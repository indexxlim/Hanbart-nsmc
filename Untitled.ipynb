{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7adbe15e-8db5-45f7-9c8e-1b07e7e1f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4519e34-18a9-4513-90bb-80e6613079ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW, BartForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "from tokenization_hanbert import HanBertTokenizer\n",
    "from kobart import get_pytorch_kobart_model, get_kobart_tokenizer\n",
    "from nsmcDatamodule import NsmcDataModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb290a1c-4ebb-4c80-ae0d-b1263a75db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'model_name': 'HanBart-54kN',\n",
    "    'random_seed': 42, # Random Seed\n",
    "    'pretrained_model': 'beomi/kcbert-base',  # Transformers PLM name\n",
    "    'pretrained_tokenizer': '',  # Optional, Transformers Tokenizer Name. Overrides `pretrained_model`\n",
    "    'batch_size': 32,\n",
    "    'lr': 5e-6,  # Starting Learning Rate\n",
    "    'epochs': 20,  # Max Epochs\n",
    "    'max_length': 150,  # Max Length input size\n",
    "    'train_data_path': \"../nsmc/ratings_train.txt\",  # Train Dataset file \n",
    "    'val_data_path': \"../nsmc/ratings_test.txt\",  # Validation Dataset file \n",
    "    'test_mode': True,  # Test Mode enables `fast_dev_run`\n",
    "    'optimizer': 'AdamW',  # AdamW vs AdamP\n",
    "    'lr_scheduler': 'exp',  # ExponentialLR vs CosineAnnealingWarmRestarts\n",
    "    'fp16': False,  # Enable train on FP16\n",
    "    'tpu_cores': 0,  # Enable TPU with 1 core or 8 cores\n",
    "    'cpu_workers': 4,\n",
    "    'max_len': 200,\n",
    "    'data_len': 150000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5a5a7-1327-46d7-a827-ff8f401e8b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6cb0796-68dc-471f-aaa1-bd729c304819",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"nsmc/ratings_train.txt\"\n",
    "test_path = \"nsmc/ratings_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bee82f-05fb-4780-9d03-2efc3676b37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44848e92-ab0f-45ba-9f52-2a8226495a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'HanBertTokenizer'.\n",
      "Some weights of the model checkpoint at ../model_checkpoint/HanBart_202110220849/saved_checkpoint_350 were not used when initializing BartForSequenceClassification: ['final_logits_bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at ../model_checkpoint/HanBart_202110220849/saved_checkpoint_350 and are newly initialized: ['classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = HanBertTokenizer.from_pretrained('HanBart-54kN')\n",
    "model = BartForSequenceClassification.from_pretrained('../model_checkpoint/HanBart_202110220849/saved_checkpoint_350', num_labels=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965bcfc3-6cbe-4488-891e-0230e2d5792c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1412bd05-f4e8-4e9a-993f-9f39457da697",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = NsmcDataModule(args['train_data_path'], args['val_data_path'], tokenizer, batch_size=args['batch_size'], max_len=args['max_len'])\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02298e18-a498-40df-ab30-9823248acc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5764f6f8-8a2e-43df-b764-94e41b754dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd0ec8b7-b3fe-4101-b863-541efd761796",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask, labels =batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9fce164-35f6-4f3f-b875-7c317ff8c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers 4.0.0+\n",
    "loss = output.loss\n",
    "logits = output.logits\n",
    "\n",
    "preds = logits.argmax(dim=-1)\n",
    "\n",
    "y_true = list(labels.cpu().numpy())\n",
    "y_pred = list(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0c47e83-5e7c-4593-89ea-f117e80bfe4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5de36-3f72-467d-bb38-797a12ff635f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
